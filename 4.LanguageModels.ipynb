{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "Dataset used: Gutenberg corpus first book.\n",
    "\n",
    "First tokenization commands usage is demonstrated. What is produced is of type 'generator' but can be converted to type list, each entry of which will be of type 'tuple'.\n",
    "\n",
    "Next the probability of the sentence \"It was a\" is calculated using unigram, bigram and trigram techniques for comparatative contrast.\n",
    "For example for bigram estimation:\n",
    "P(w<sub>i</sub> |w<sub>i-1</sub>)= Count(w<sub>i</sub>,w<sub>i-1</sub>)/Count(w<sub>i-1</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the bigram list is 191672\n",
      "Length of the trigram list is 191671\n",
      "Type of each bigram entry is <type 'tuple'>\n",
      "In unigram model Probability of sentence = 0.000002\n",
      "In bigram model Probability of sentence = 0.008280\n",
      "In trigram model Probability of sentence = 0.078014\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "token=nltk.word_tokenize(nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids()[0]))\n",
    "#print type(ngrams(token,2))\n",
    "bigrams=list(ngrams(token,2))\n",
    "trigrams=list(ngrams(token,3))\n",
    "print \"Length of the bigram list is %d\" %(len(bigrams))\n",
    "print \"Length of the trigram list is %d\" %(len(trigrams))\n",
    "print \"Type of each bigram entry is %s\" %(type(bigrams[0]))\n",
    "\n",
    "#Now Calculating probability of \"It will prove\"\n",
    "#Approach 1 : Unigram \n",
    "print \"In unigram model Probability of sentence = %f\" %(token.count(\"it\")* token.count(\"was\")* token.count(\"a\")/float(len(token)*len(token)*len(token)))\n",
    "#Approach 2 : Bigram\n",
    "count1=0\n",
    "count2=0\n",
    "for item in bigrams:\n",
    "  if (\"it was\" == ' '.join(item)):\n",
    "      count1=count1+1\n",
    "  if (\"was a\" == ' '.join(item)):\n",
    "      count2=count2+1\n",
    "print \"In bigram model Probability of sentence = %f\" %(count1* count2/float(token.count(\"it\")* token.count(\"was\")))\n",
    "#Approach 3 Trigrams\n",
    "count2=0\n",
    "for item in trigrams:\n",
    "  if (\"it was a\" == ' '.join(item)):\n",
    "      count2=count2+1\n",
    "print \"In trigram model Probability of sentence = %f\" %(count2/float(count1))\n",
    "#print trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "Assume you have a speech2text converted utterance. the text obtained<br> \n",
    "has no word markersin between. You have to put the marker at correct\n",
    "places.\n",
    "\n",
    "Can you give this a try? can you try ro find meanningful words so that \n",
    "the sequence given below becomes a meaningful sentence\n",
    "\n",
    "\n",
    "- \"asgregorsamsaawokeonemorningfromuneasydreamshefoundhimself<br>transformedinhisbedintoagiganticinsect\"\n",
    "\n",
    "- \"choosespain\" - choose spain\n",
    "\n",
    "\n",
    "### Solution \n",
    "\n",
    "- **Define a probabilistic model.** We can’t define all the factors \n",
    "(semantic, syntactic, lexical, and social) that make “choose Spain” <br>\n",
    "a better candidate for a domain name, but we can define a simplified model<br>\n",
    "that gives approximate probabilities. For short candidates like “choose Spain”<br>\n",
    "we could just look up the n-gram in the corpus data and use that as the probability.<br>\n",
    "For longer candidates we will need some way of composing an answer from smaller parts.<br>\n",
    "For words we haven’t seen before, we’ll have to estimate the probability of an unknown word.<br>\n",
    "The point is that we define a language model—a probability distribution over all the <br>\n",
    "strings in the language—and learn the parameters of the model from our corpus data, <br>\n",
    "then use the model to define the probability of each candidate.\n",
    "\n",
    "- **Enumerate candidates.** We may not be sure whether “insufficient numbers” or “in sufficient numbers” is more likely to be the intended phrase, but we can agree that they are both candidate segmentations, as is “in suffi cient numb ers,” but that “hello world” is not a valid candidate. In this step we withhold judgment and just enumerate possibilities—all the \n",
    "possibilities if we can, or else a carefully selected sample.\n",
    "\n",
    "- **Choose the most probable candidate.** Apply the language model to each \n",
    "candidate to get its probability, and choose the one with the highest \n",
    "probability.\n",
    "\n",
    "$$ best = argmax_{c ∈ candidates}        P(c) $$\n",
    "\n",
    "\n",
    "#### unigram model\n",
    "$$ P(W_{1:n} ) = Π_{k=1:n} P(W_{k} ) $$\n",
    "\n",
    "#### ngram model\n",
    "\n",
    "$$ P(W_{1:n} ) = Π_{k=1:n} P(W_{k} | W_{1:k–1} ) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string, random, glob, operator, heapq\n",
    "from collections import defaultdict\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            # obtains the count of a  word occurence\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.itervalues()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        #calculates the probability of the word by dividing by the total occurence\n",
    "        # when the object of Pdist is called returns the probability value\n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "# Generator function, which returns each line as a list wher efirst entry is the word\n",
    "#Second entry is the count. Basically split by space\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    for line in file(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "def avoid_long_words(key, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N * 10**len(key))\n",
    "\n",
    "# ------------ Execution Starts Here ----------------- #\n",
    "# The trillion-word data set was published by Thorsten Brants and Alex Franz of Google in 2006\n",
    "\n",
    "N = 1024908267229 ## Number of tokens\n",
    "\n",
    "# count_1w.txt has unirams and their occurence (tab separated) in a corpus\n",
    "# Each word is stored as the word value \n",
    "# avoid_long_words is a function (defined above) that does smoothing, i.e.e when a OOV word is found.\n",
    "Pw  = Pdist(datafile('count_1w.txt'), N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#memoization - python decorator\n",
    "# does the memoization part i.e. table lookup\n",
    "def memo(f):\n",
    "    \"Memoize function f.\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo\n",
    "\n",
    "def test(verbose=None):\n",
    "    \"\"\"Run some tests, taken from the chapter.\n",
    "    Since the hillclimbing algorithm is randomized, some tests may fail.\"\"\"\n",
    "    import doctest\n",
    "    print 'Running tests...'\n",
    "    doctest.testfile('ngrams-test.txt', verbose=verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "- Look into the decorator memo used in the cel below as @memo\n",
    "\n",
    "- Without memo , a call to the funciton segment for an n-character text makes $ 2^{n} $ recursive calls to segment ;\n",
    "- with memo it makes only n calls— memo makes this a fairly efficient dynamic programming algorithm. Each of the n calls considers O(L) splits, and evaluates each split by multiplying O(n) probabilities, so the whole algorithm is O(n^{2}L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.0225735823407\n",
      "hello 3.21593473815e-05\n",
      "services 0.000548543535042\n",
      "['choose', 'spain']\n",
      "['when', 'in', 'the', 'course', 'of', 'human', 'events', 'it', 'becomes', 'necessary']\n"
     ]
    }
   ],
   "source": [
    "################ Word Segmentation (p. 223)\n",
    "\n",
    "\n",
    "\n",
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the best segmentation of text.\"\n",
    "    if not text: return []\n",
    "    # find the candidates using segment function. \n",
    "    candidates = ([first]+segment(rem) for first,rem in splits(text))\n",
    "    # find the maximum likelihood\n",
    "    return max(candidates, key=Pwords)\n",
    "\n",
    "def splits(text, L=20):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    return [(text[:i+1], text[i+1:]) \n",
    "            for i in range(min(len(text), L))]\n",
    "\n",
    "def Pwords(words): \n",
    "    \"The Naive Bayes probability of a sequence of words.\"\n",
    "    return product(Pw(w) for w in words)\n",
    "# Probabiliy of words\n",
    "\n",
    "\n",
    "print 'the',Pw('the')\n",
    "print 'hello', Pw('hello')\n",
    "print 'services', Pw('services')\n",
    "\n",
    "print segment('choosespain')\n",
    "# one in 35 trillion segmentations\n",
    "print segment('wheninthecourseofhumaneventsitbecomesnecessary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-24.959286263365577, ['when', 'in', 'the', 'course', 'of', 'human', 'events', 'it', 'becomes', 'necessary'])\n",
      "(-76.5578290219908, ['as', 'gregor', 'samsa', 'awoke', 'one', 'morning', 'from', 'uneasy', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'gigantic', 'insect'])\n"
     ]
    }
   ],
   "source": [
    "# Bigram model\n",
    "\n",
    "def cPw(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    try:\n",
    "        return P2w[prev + ' ' + word]/float(Pw[prev])\n",
    "    except KeyError:\n",
    "        return Pw(word)\n",
    "\n",
    "P2w = Pdist(datafile('count_2w.txt'), N)\n",
    "\n",
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Return (log P(words), words), where words is the best segmentation.\" \n",
    "    # <S> is to handle starting words\n",
    "    if not text: return 0.0, [] \n",
    "    candidates = [combine(log10(cPw(first, prev)), first, segment2(rem, first)) \n",
    "                  for first,rem in splits(text)] \n",
    "    return max(candidates) \n",
    "\n",
    "def combine(Pfirst, first, (Prem, rem)): \n",
    "    \"Combine first and rem results into one (probability, words) pair.\" \n",
    "    return Pfirst+Prem, [first]+rem\n",
    "\n",
    "print segment2('wheninthecourseofhumaneventsitbecomesnecessary')\n",
    "print segment2('asgregorsamsaawokeonemorningfromuneasydreamshefoundhimselftransformed\\\n",
    "inhisbedintoagiganticinsect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
